{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import pickle\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
    "    installed).\n",
    "\n",
    "    Args:\n",
    "        seed (:obj:`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if is_torch_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "    if is_tf_available():\n",
    "        import tensorflow as tf\n",
    "\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"bert-base-chinese\"#bert-base-chinese bert-base-uncased\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'chinese'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### big model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in chinese/tokenizer\\tokenizer_config.json\n",
      "Special tokens file saved in chinese/tokenizer\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv('twitter_normal_data.csv', header = None)\n",
    "\n",
    "import pickle\n",
    "with open('comment_final.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "    df.columns = [0,1]\n",
    "emojis = df[1].unique()\n",
    "for i in range(len(emojis)):\n",
    "    if len(df[df[1] == emojis[i]]) <= 10:\n",
    "        df = df.drop(df[df[1] == emojis[i]].index, axis = 0)\n",
    "emoji_dict = {}\n",
    "emojis = df[1].unique()\n",
    "for i in range(len(emojis)):\n",
    "    emoji_dict[emojis[i]] = i\n",
    "tokenizer.add_tokens(list(emojis))\n",
    "tokenizer.save_pretrained(lang + '/tokenizer')\n",
    "\n",
    "df[1] = df[1].replace(emoji_dict)\n",
    "with open(lang + '/emoji_dict.pickle', 'wb') as f:\n",
    "     pickle.dump(emoji_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customized model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all.csv')\n",
    "df.columns = [0,1,2]\n",
    "\n",
    "with open(lang +'/emoji_dict.pickle', 'rb') as f:\n",
    "     emoji_dict = pickle.load(f)\n",
    "df[1] = df[1].replace(emoji_dict)\n",
    "df  = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(emoji_dict)):\n",
    "    if len(df[df[1] == i]) == 1:\n",
    "        df = df.drop(df[df[1] == i].index, axis = 0)\n",
    "    if len(df[df[1] == i]) == 0:\n",
    "        df = df.append(pd.DataFrame([['',i,''],['',i,'']]), ignore_index=True)\n",
    "df = df[df[1].str.isnumeric().fillna(True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./chinese/original-19500\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"checkpoint-15500\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\",\n",
      "    \"156\": \"LABEL_156\",\n",
      "    \"157\": \"LABEL_157\",\n",
      "    \"158\": \"LABEL_158\",\n",
      "    \"159\": \"LABEL_159\",\n",
      "    \"160\": \"LABEL_160\",\n",
      "    \"161\": \"LABEL_161\",\n",
      "    \"162\": \"LABEL_162\",\n",
      "    \"163\": \"LABEL_163\",\n",
      "    \"164\": \"LABEL_164\",\n",
      "    \"165\": \"LABEL_165\",\n",
      "    \"166\": \"LABEL_166\",\n",
      "    \"167\": \"LABEL_167\",\n",
      "    \"168\": \"LABEL_168\",\n",
      "    \"169\": \"LABEL_169\",\n",
      "    \"170\": \"LABEL_170\",\n",
      "    \"171\": \"LABEL_171\",\n",
      "    \"172\": \"LABEL_172\",\n",
      "    \"173\": \"LABEL_173\",\n",
      "    \"174\": \"LABEL_174\",\n",
      "    \"175\": \"LABEL_175\",\n",
      "    \"176\": \"LABEL_176\",\n",
      "    \"177\": \"LABEL_177\",\n",
      "    \"178\": \"LABEL_178\",\n",
      "    \"179\": \"LABEL_179\",\n",
      "    \"180\": \"LABEL_180\",\n",
      "    \"181\": \"LABEL_181\",\n",
      "    \"182\": \"LABEL_182\",\n",
      "    \"183\": \"LABEL_183\",\n",
      "    \"184\": \"LABEL_184\",\n",
      "    \"185\": \"LABEL_185\",\n",
      "    \"186\": \"LABEL_186\",\n",
      "    \"187\": \"LABEL_187\",\n",
      "    \"188\": \"LABEL_188\",\n",
      "    \"189\": \"LABEL_189\",\n",
      "    \"190\": \"LABEL_190\",\n",
      "    \"191\": \"LABEL_191\",\n",
      "    \"192\": \"LABEL_192\",\n",
      "    \"193\": \"LABEL_193\",\n",
      "    \"194\": \"LABEL_194\",\n",
      "    \"195\": \"LABEL_195\",\n",
      "    \"196\": \"LABEL_196\",\n",
      "    \"197\": \"LABEL_197\",\n",
      "    \"198\": \"LABEL_198\",\n",
      "    \"199\": \"LABEL_199\",\n",
      "    \"200\": \"LABEL_200\",\n",
      "    \"201\": \"LABEL_201\",\n",
      "    \"202\": \"LABEL_202\",\n",
      "    \"203\": \"LABEL_203\",\n",
      "    \"204\": \"LABEL_204\",\n",
      "    \"205\": \"LABEL_205\",\n",
      "    \"206\": \"LABEL_206\",\n",
      "    \"207\": \"LABEL_207\",\n",
      "    \"208\": \"LABEL_208\",\n",
      "    \"209\": \"LABEL_209\",\n",
      "    \"210\": \"LABEL_210\",\n",
      "    \"211\": \"LABEL_211\",\n",
      "    \"212\": \"LABEL_212\",\n",
      "    \"213\": \"LABEL_213\",\n",
      "    \"214\": \"LABEL_214\",\n",
      "    \"215\": \"LABEL_215\",\n",
      "    \"216\": \"LABEL_216\",\n",
      "    \"217\": \"LABEL_217\",\n",
      "    \"218\": \"LABEL_218\",\n",
      "    \"219\": \"LABEL_219\",\n",
      "    \"220\": \"LABEL_220\",\n",
      "    \"221\": \"LABEL_221\",\n",
      "    \"222\": \"LABEL_222\",\n",
      "    \"223\": \"LABEL_223\",\n",
      "    \"224\": \"LABEL_224\",\n",
      "    \"225\": \"LABEL_225\",\n",
      "    \"226\": \"LABEL_226\",\n",
      "    \"227\": \"LABEL_227\",\n",
      "    \"228\": \"LABEL_228\",\n",
      "    \"229\": \"LABEL_229\",\n",
      "    \"230\": \"LABEL_230\",\n",
      "    \"231\": \"LABEL_231\",\n",
      "    \"232\": \"LABEL_232\",\n",
      "    \"233\": \"LABEL_233\",\n",
      "    \"234\": \"LABEL_234\",\n",
      "    \"235\": \"LABEL_235\",\n",
      "    \"236\": \"LABEL_236\",\n",
      "    \"237\": \"LABEL_237\",\n",
      "    \"238\": \"LABEL_238\",\n",
      "    \"239\": \"LABEL_239\",\n",
      "    \"240\": \"LABEL_240\",\n",
      "    \"241\": \"LABEL_241\",\n",
      "    \"242\": \"LABEL_242\",\n",
      "    \"243\": \"LABEL_243\",\n",
      "    \"244\": \"LABEL_244\",\n",
      "    \"245\": \"LABEL_245\",\n",
      "    \"246\": \"LABEL_246\",\n",
      "    \"247\": \"LABEL_247\",\n",
      "    \"248\": \"LABEL_248\",\n",
      "    \"249\": \"LABEL_249\",\n",
      "    \"250\": \"LABEL_250\",\n",
      "    \"251\": \"LABEL_251\",\n",
      "    \"252\": \"LABEL_252\",\n",
      "    \"253\": \"LABEL_253\",\n",
      "    \"254\": \"LABEL_254\",\n",
      "    \"255\": \"LABEL_255\",\n",
      "    \"256\": \"LABEL_256\",\n",
      "    \"257\": \"LABEL_257\",\n",
      "    \"258\": \"LABEL_258\",\n",
      "    \"259\": \"LABEL_259\",\n",
      "    \"260\": \"LABEL_260\",\n",
      "    \"261\": \"LABEL_261\",\n",
      "    \"262\": \"LABEL_262\",\n",
      "    \"263\": \"LABEL_263\",\n",
      "    \"264\": \"LABEL_264\",\n",
      "    \"265\": \"LABEL_265\",\n",
      "    \"266\": \"LABEL_266\",\n",
      "    \"267\": \"LABEL_267\",\n",
      "    \"268\": \"LABEL_268\",\n",
      "    \"269\": \"LABEL_269\",\n",
      "    \"270\": \"LABEL_270\",\n",
      "    \"271\": \"LABEL_271\",\n",
      "    \"272\": \"LABEL_272\",\n",
      "    \"273\": \"LABEL_273\",\n",
      "    \"274\": \"LABEL_274\",\n",
      "    \"275\": \"LABEL_275\",\n",
      "    \"276\": \"LABEL_276\",\n",
      "    \"277\": \"LABEL_277\",\n",
      "    \"278\": \"LABEL_278\",\n",
      "    \"279\": \"LABEL_279\",\n",
      "    \"280\": \"LABEL_280\",\n",
      "    \"281\": \"LABEL_281\",\n",
      "    \"282\": \"LABEL_282\",\n",
      "    \"283\": \"LABEL_283\",\n",
      "    \"284\": \"LABEL_284\",\n",
      "    \"285\": \"LABEL_285\",\n",
      "    \"286\": \"LABEL_286\",\n",
      "    \"287\": \"LABEL_287\",\n",
      "    \"288\": \"LABEL_288\",\n",
      "    \"289\": \"LABEL_289\",\n",
      "    \"290\": \"LABEL_290\",\n",
      "    \"291\": \"LABEL_291\",\n",
      "    \"292\": \"LABEL_292\",\n",
      "    \"293\": \"LABEL_293\",\n",
      "    \"294\": \"LABEL_294\",\n",
      "    \"295\": \"LABEL_295\",\n",
      "    \"296\": \"LABEL_296\",\n",
      "    \"297\": \"LABEL_297\",\n",
      "    \"298\": \"LABEL_298\",\n",
      "    \"299\": \"LABEL_299\",\n",
      "    \"300\": \"LABEL_300\",\n",
      "    \"301\": \"LABEL_301\",\n",
      "    \"302\": \"LABEL_302\",\n",
      "    \"303\": \"LABEL_303\",\n",
      "    \"304\": \"LABEL_304\",\n",
      "    \"305\": \"LABEL_305\",\n",
      "    \"306\": \"LABEL_306\",\n",
      "    \"307\": \"LABEL_307\",\n",
      "    \"308\": \"LABEL_308\",\n",
      "    \"309\": \"LABEL_309\",\n",
      "    \"310\": \"LABEL_310\",\n",
      "    \"311\": \"LABEL_311\",\n",
      "    \"312\": \"LABEL_312\",\n",
      "    \"313\": \"LABEL_313\",\n",
      "    \"314\": \"LABEL_314\",\n",
      "    \"315\": \"LABEL_315\",\n",
      "    \"316\": \"LABEL_316\",\n",
      "    \"317\": \"LABEL_317\",\n",
      "    \"318\": \"LABEL_318\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_156\": 156,\n",
      "    \"LABEL_157\": 157,\n",
      "    \"LABEL_158\": 158,\n",
      "    \"LABEL_159\": 159,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_160\": 160,\n",
      "    \"LABEL_161\": 161,\n",
      "    \"LABEL_162\": 162,\n",
      "    \"LABEL_163\": 163,\n",
      "    \"LABEL_164\": 164,\n",
      "    \"LABEL_165\": 165,\n",
      "    \"LABEL_166\": 166,\n",
      "    \"LABEL_167\": 167,\n",
      "    \"LABEL_168\": 168,\n",
      "    \"LABEL_169\": 169,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_170\": 170,\n",
      "    \"LABEL_171\": 171,\n",
      "    \"LABEL_172\": 172,\n",
      "    \"LABEL_173\": 173,\n",
      "    \"LABEL_174\": 174,\n",
      "    \"LABEL_175\": 175,\n",
      "    \"LABEL_176\": 176,\n",
      "    \"LABEL_177\": 177,\n",
      "    \"LABEL_178\": 178,\n",
      "    \"LABEL_179\": 179,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_180\": 180,\n",
      "    \"LABEL_181\": 181,\n",
      "    \"LABEL_182\": 182,\n",
      "    \"LABEL_183\": 183,\n",
      "    \"LABEL_184\": 184,\n",
      "    \"LABEL_185\": 185,\n",
      "    \"LABEL_186\": 186,\n",
      "    \"LABEL_187\": 187,\n",
      "    \"LABEL_188\": 188,\n",
      "    \"LABEL_189\": 189,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_190\": 190,\n",
      "    \"LABEL_191\": 191,\n",
      "    \"LABEL_192\": 192,\n",
      "    \"LABEL_193\": 193,\n",
      "    \"LABEL_194\": 194,\n",
      "    \"LABEL_195\": 195,\n",
      "    \"LABEL_196\": 196,\n",
      "    \"LABEL_197\": 197,\n",
      "    \"LABEL_198\": 198,\n",
      "    \"LABEL_199\": 199,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_200\": 200,\n",
      "    \"LABEL_201\": 201,\n",
      "    \"LABEL_202\": 202,\n",
      "    \"LABEL_203\": 203,\n",
      "    \"LABEL_204\": 204,\n",
      "    \"LABEL_205\": 205,\n",
      "    \"LABEL_206\": 206,\n",
      "    \"LABEL_207\": 207,\n",
      "    \"LABEL_208\": 208,\n",
      "    \"LABEL_209\": 209,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_210\": 210,\n",
      "    \"LABEL_211\": 211,\n",
      "    \"LABEL_212\": 212,\n",
      "    \"LABEL_213\": 213,\n",
      "    \"LABEL_214\": 214,\n",
      "    \"LABEL_215\": 215,\n",
      "    \"LABEL_216\": 216,\n",
      "    \"LABEL_217\": 217,\n",
      "    \"LABEL_218\": 218,\n",
      "    \"LABEL_219\": 219,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_220\": 220,\n",
      "    \"LABEL_221\": 221,\n",
      "    \"LABEL_222\": 222,\n",
      "    \"LABEL_223\": 223,\n",
      "    \"LABEL_224\": 224,\n",
      "    \"LABEL_225\": 225,\n",
      "    \"LABEL_226\": 226,\n",
      "    \"LABEL_227\": 227,\n",
      "    \"LABEL_228\": 228,\n",
      "    \"LABEL_229\": 229,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_230\": 230,\n",
      "    \"LABEL_231\": 231,\n",
      "    \"LABEL_232\": 232,\n",
      "    \"LABEL_233\": 233,\n",
      "    \"LABEL_234\": 234,\n",
      "    \"LABEL_235\": 235,\n",
      "    \"LABEL_236\": 236,\n",
      "    \"LABEL_237\": 237,\n",
      "    \"LABEL_238\": 238,\n",
      "    \"LABEL_239\": 239,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_240\": 240,\n",
      "    \"LABEL_241\": 241,\n",
      "    \"LABEL_242\": 242,\n",
      "    \"LABEL_243\": 243,\n",
      "    \"LABEL_244\": 244,\n",
      "    \"LABEL_245\": 245,\n",
      "    \"LABEL_246\": 246,\n",
      "    \"LABEL_247\": 247,\n",
      "    \"LABEL_248\": 248,\n",
      "    \"LABEL_249\": 249,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_250\": 250,\n",
      "    \"LABEL_251\": 251,\n",
      "    \"LABEL_252\": 252,\n",
      "    \"LABEL_253\": 253,\n",
      "    \"LABEL_254\": 254,\n",
      "    \"LABEL_255\": 255,\n",
      "    \"LABEL_256\": 256,\n",
      "    \"LABEL_257\": 257,\n",
      "    \"LABEL_258\": 258,\n",
      "    \"LABEL_259\": 259,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_260\": 260,\n",
      "    \"LABEL_261\": 261,\n",
      "    \"LABEL_262\": 262,\n",
      "    \"LABEL_263\": 263,\n",
      "    \"LABEL_264\": 264,\n",
      "    \"LABEL_265\": 265,\n",
      "    \"LABEL_266\": 266,\n",
      "    \"LABEL_267\": 267,\n",
      "    \"LABEL_268\": 268,\n",
      "    \"LABEL_269\": 269,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_270\": 270,\n",
      "    \"LABEL_271\": 271,\n",
      "    \"LABEL_272\": 272,\n",
      "    \"LABEL_273\": 273,\n",
      "    \"LABEL_274\": 274,\n",
      "    \"LABEL_275\": 275,\n",
      "    \"LABEL_276\": 276,\n",
      "    \"LABEL_277\": 277,\n",
      "    \"LABEL_278\": 278,\n",
      "    \"LABEL_279\": 279,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_280\": 280,\n",
      "    \"LABEL_281\": 281,\n",
      "    \"LABEL_282\": 282,\n",
      "    \"LABEL_283\": 283,\n",
      "    \"LABEL_284\": 284,\n",
      "    \"LABEL_285\": 285,\n",
      "    \"LABEL_286\": 286,\n",
      "    \"LABEL_287\": 287,\n",
      "    \"LABEL_288\": 288,\n",
      "    \"LABEL_289\": 289,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_290\": 290,\n",
      "    \"LABEL_291\": 291,\n",
      "    \"LABEL_292\": 292,\n",
      "    \"LABEL_293\": 293,\n",
      "    \"LABEL_294\": 294,\n",
      "    \"LABEL_295\": 295,\n",
      "    \"LABEL_296\": 296,\n",
      "    \"LABEL_297\": 297,\n",
      "    \"LABEL_298\": 298,\n",
      "    \"LABEL_299\": 299,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_300\": 300,\n",
      "    \"LABEL_301\": 301,\n",
      "    \"LABEL_302\": 302,\n",
      "    \"LABEL_303\": 303,\n",
      "    \"LABEL_304\": 304,\n",
      "    \"LABEL_305\": 305,\n",
      "    \"LABEL_306\": 306,\n",
      "    \"LABEL_307\": 307,\n",
      "    \"LABEL_308\": 308,\n",
      "    \"LABEL_309\": 309,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_310\": 310,\n",
      "    \"LABEL_311\": 311,\n",
      "    \"LABEL_312\": 312,\n",
      "    \"LABEL_313\": 313,\n",
      "    \"LABEL_314\": 314,\n",
      "    \"LABEL_315\": 315,\n",
      "    \"LABEL_316\": 316,\n",
      "    \"LABEL_317\": 317,\n",
      "    \"LABEL_318\": 318,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 22082\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file ./chinese/original-19500\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./chinese/original-19500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file chinese/tokenizer\\vocab.txt\n",
      "loading file chinese/tokenizer\\tokenizer.json\n",
      "loading file chinese/tokenizer\\added_tokens.json\n",
      "loading file chinese/tokenizer\\special_tokens_map.json\n",
      "loading file chinese/tokenizer\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "model_name = './chinese/original-19500'#./chinese/customize-14500\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=319).to(\"cuda\")#len(df[1].unique())\n",
    "tokenizer = AutoTokenizer.from_pretrained(lang + '/tokenizer')\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text, trainY, testY = train_test_split(df[0].values, df[1].values, test_size=0.2, shuffle = True, stratify = df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = tokenizer(train_text.tolist(), truncation=True, padding=True, max_length=512)\n",
    "testX = tokenizer(test_text.tolist(), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# convert our tokenized data into a torch Dataset\n",
    "\n",
    "train_dataset = ReviewDataset(trainX, trainY)\n",
    "test_dataset = ReviewDataset(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro = f1_score(labels, preds, average='macro')\n",
    "    micro = f1_score(labels, preds, average='micro')\n",
    "    weighted = f1_score(labels, preds, average='weighted')\n",
    "    k = 5\n",
    "    preds_k = torch.topk(torch.tensor(pred.predictions), k).indices.cpu().numpy()\n",
    "    c=0\n",
    "    for i,l in enumerate(labels):\n",
    "        if l in preds_k[i]:\n",
    "            c += 1\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "        'macro': macro,\n",
    "        'micro': micro,\n",
    "        'weighted': weighted,\n",
    "        'acc_at_k': c/len(labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='D:/reviewsData/result/chinese/customized',          # output directory\n",
    "    num_train_epochs=9,              # total number of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=4,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "    logging_steps=500,               # log & save weights each logging_steps\n",
    "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 6811\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15327\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15165' max='15327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15165/15327 2:05:52 < 01:20, 2.01 it/s, Epoch 8.90/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro</th>\n",
       "      <th>Micro</th>\n",
       "      <th>Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.436300</td>\n",
       "      <td>2.791839</td>\n",
       "      <td>0.604815</td>\n",
       "      <td>0.417247</td>\n",
       "      <td>0.604815</td>\n",
       "      <td>0.592545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.719200</td>\n",
       "      <td>2.813612</td>\n",
       "      <td>0.595420</td>\n",
       "      <td>0.407451</td>\n",
       "      <td>0.595420</td>\n",
       "      <td>0.585283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.753200</td>\n",
       "      <td>2.830524</td>\n",
       "      <td>0.598943</td>\n",
       "      <td>0.410634</td>\n",
       "      <td>0.598943</td>\n",
       "      <td>0.584184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.729100</td>\n",
       "      <td>2.774474</td>\n",
       "      <td>0.593658</td>\n",
       "      <td>0.408865</td>\n",
       "      <td>0.593658</td>\n",
       "      <td>0.580663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.731700</td>\n",
       "      <td>2.802470</td>\n",
       "      <td>0.596007</td>\n",
       "      <td>0.403153</td>\n",
       "      <td>0.596007</td>\n",
       "      <td>0.584797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.673400</td>\n",
       "      <td>2.879263</td>\n",
       "      <td>0.597769</td>\n",
       "      <td>0.409481</td>\n",
       "      <td>0.597769</td>\n",
       "      <td>0.588098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.657700</td>\n",
       "      <td>2.879191</td>\n",
       "      <td>0.599530</td>\n",
       "      <td>0.418100</td>\n",
       "      <td>0.599530</td>\n",
       "      <td>0.588382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.572900</td>\n",
       "      <td>2.808121</td>\n",
       "      <td>0.603641</td>\n",
       "      <td>0.427583</td>\n",
       "      <td>0.603641</td>\n",
       "      <td>0.598950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.649400</td>\n",
       "      <td>2.896103</td>\n",
       "      <td>0.601292</td>\n",
       "      <td>0.406254</td>\n",
       "      <td>0.601292</td>\n",
       "      <td>0.588419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.627500</td>\n",
       "      <td>2.944963</td>\n",
       "      <td>0.596594</td>\n",
       "      <td>0.403492</td>\n",
       "      <td>0.596594</td>\n",
       "      <td>0.583616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.469600</td>\n",
       "      <td>2.982610</td>\n",
       "      <td>0.606577</td>\n",
       "      <td>0.424379</td>\n",
       "      <td>0.606577</td>\n",
       "      <td>0.600611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.545900</td>\n",
       "      <td>2.867413</td>\n",
       "      <td>0.608338</td>\n",
       "      <td>0.434179</td>\n",
       "      <td>0.608338</td>\n",
       "      <td>0.598558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.629700</td>\n",
       "      <td>2.853428</td>\n",
       "      <td>0.604815</td>\n",
       "      <td>0.420694</td>\n",
       "      <td>0.604815</td>\n",
       "      <td>0.590205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>2.937269</td>\n",
       "      <td>0.597181</td>\n",
       "      <td>0.427113</td>\n",
       "      <td>0.597181</td>\n",
       "      <td>0.595061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.443000</td>\n",
       "      <td>2.976248</td>\n",
       "      <td>0.602466</td>\n",
       "      <td>0.419633</td>\n",
       "      <td>0.602466</td>\n",
       "      <td>0.595113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>2.874936</td>\n",
       "      <td>0.607164</td>\n",
       "      <td>0.420876</td>\n",
       "      <td>0.607164</td>\n",
       "      <td>0.594428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.575200</td>\n",
       "      <td>2.865125</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.415753</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.597789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.390200</td>\n",
       "      <td>2.993411</td>\n",
       "      <td>0.605402</td>\n",
       "      <td>0.430032</td>\n",
       "      <td>0.605402</td>\n",
       "      <td>0.599447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.450100</td>\n",
       "      <td>2.810302</td>\n",
       "      <td>0.610100</td>\n",
       "      <td>0.426025</td>\n",
       "      <td>0.610100</td>\n",
       "      <td>0.601434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.470900</td>\n",
       "      <td>2.939310</td>\n",
       "      <td>0.608925</td>\n",
       "      <td>0.427654</td>\n",
       "      <td>0.608925</td>\n",
       "      <td>0.596102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.429200</td>\n",
       "      <td>3.065842</td>\n",
       "      <td>0.610100</td>\n",
       "      <td>0.430421</td>\n",
       "      <td>0.610100</td>\n",
       "      <td>0.602195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.403100</td>\n",
       "      <td>2.995183</td>\n",
       "      <td>0.611274</td>\n",
       "      <td>0.429248</td>\n",
       "      <td>0.611274</td>\n",
       "      <td>0.600405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.438000</td>\n",
       "      <td>2.858607</td>\n",
       "      <td>0.610687</td>\n",
       "      <td>0.426721</td>\n",
       "      <td>0.610687</td>\n",
       "      <td>0.600380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>2.951632</td>\n",
       "      <td>0.610687</td>\n",
       "      <td>0.417008</td>\n",
       "      <td>0.610687</td>\n",
       "      <td>0.596498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>2.964910</td>\n",
       "      <td>0.611274</td>\n",
       "      <td>0.429092</td>\n",
       "      <td>0.611274</td>\n",
       "      <td>0.600176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.376500</td>\n",
       "      <td>2.943771</td>\n",
       "      <td>0.609513</td>\n",
       "      <td>0.425678</td>\n",
       "      <td>0.609513</td>\n",
       "      <td>0.597680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.404900</td>\n",
       "      <td>2.929048</td>\n",
       "      <td>0.611274</td>\n",
       "      <td>0.423082</td>\n",
       "      <td>0.611274</td>\n",
       "      <td>0.598966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.349500</td>\n",
       "      <td>2.944276</td>\n",
       "      <td>0.613623</td>\n",
       "      <td>0.423528</td>\n",
       "      <td>0.613623</td>\n",
       "      <td>0.601518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>2.952226</td>\n",
       "      <td>0.614797</td>\n",
       "      <td>0.427857</td>\n",
       "      <td>0.614797</td>\n",
       "      <td>0.602988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>2.951131</td>\n",
       "      <td>0.612449</td>\n",
       "      <td>0.427387</td>\n",
       "      <td>0.612449</td>\n",
       "      <td>0.601623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-1000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-1000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-1500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-1500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-2000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-2000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-2500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-2500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-3000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-3000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-3500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-3500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-3500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-4000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-4000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-4000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-4500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-4500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-5000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-5000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-5000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-5500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-5500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-5500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-6000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-6000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-6000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-6500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-6500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-6500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-7000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-7000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-7000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-7500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-7500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-7500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-8000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-8000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-8000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-8500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-8500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-8500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-9000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-9000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-9000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-9500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-9500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-9500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-10000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-10000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-10000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-10500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-10500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-10500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-11000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-11000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-11000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-11500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-11500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-11500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-12000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-12000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-12000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-12500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-12500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-12500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-13000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-13000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-13000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-13500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-13500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-13500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-14000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-14000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-14000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-14500\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-14500\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-14500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1703\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to D:/reviewsData/result/chinese/customized\\checkpoint-15000\n",
      "Configuration saved in D:/reviewsData/result/chinese/customized\\checkpoint-15000\\config.json\n",
      "Model weights saved in D:/reviewsData/result/chinese/customized\\checkpoint-15000\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6659\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1665' max='1665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1665/1665 00:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2417843341827393,\n",
       " 'eval_accuracy': 0.709866346298243,\n",
       " 'eval_macro': 0.5921293629851295,\n",
       " 'eval_micro': 0.709866346298243,\n",
       " 'eval_weighted': 0.6974557179800941,\n",
       " 'eval_acc_at_k': 0.8720528607899084,\n",
       " 'eval_runtime': 56.9387,\n",
       " 'eval_samples_per_second': 116.95,\n",
       " 'eval_steps_per_second': 29.242}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = torch.load('checkpoint-19500/training_args.bin')\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(text):\n",
    "    # prepare our text into tokenized sequence\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # perform inference to our model\n",
    "    outputs = model(**inputs)\n",
    "    # get output probabilities by doing softmax\n",
    "    probs = outputs[0].softmax(1)\n",
    "    # executing argmax function to get the candidate label\n",
    "    return target_names[probs.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example #1\n",
    "text = \"\"\"\n",
    "The first thing is first. \n",
    "If you purchase a Macbook, you should not encounter performance issues that will prevent you from learning to code efficiently.\n",
    "However, in the off chance that you have to deal with a slow computer, you will need to make some adjustments. \n",
    "Having too many background apps running in the background is one of the most common causes. \n",
    "The same can be said about a lack of drive storage. \n",
    "For that, it helps if you uninstall xcode and other unnecessary applications, as well as temporary system junk like caches and old backups.\n",
    "\"\"\"\n",
    "print(get_prediction(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
